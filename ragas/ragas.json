[{"answer":"Based on the provided context, here is how `swiftide` is used:\n\n```markdown\nswiftide is used in the context of handling file loading, indexing, and querying. Specifically, it is used to:\n\n1. Load files with specific extensions including `md` using `FileLoader`.\n2. Set up an indexing pipeline (`Pipeline`) that processes these files with concurrency and caching capabilities.\n3. Split the loaded content into Markdown and code sections.\n4. Process code sections with `ChunkCode` and `MetadataQACode`.\n5. Process Markdown sections with `ChunkMarkdown` and `MetadataQAText`.\n6. Merge the processed content and embed it using `Embed`, then store it with `qdrant`.\n7. Index content for querying.\n8. Query the indexed content using a pipeline that evaluates, transforms, retrieves, and answers questions.\n\nKey components of `swiftide` used in this context include:\n\n- `indexing::Pipeline`\n- `indexing::loaders::FileLoader`\n- `indexing::transformers::{ChunkCode, ChunkMarkdown, Embed, MetadataQACode, MetadataQAText}`\n- `query::{Pipeline, evaluators, query_transformers, Query, states, answers::Simple}`\n```\n\nThis context does not provide details on the internal implementations but clearly outlines the steps and components involving the `swiftide` library.","contexts":["\"\\n\\n    let language = SupportedLanguages::from_str(language)?;\\n    let mut extensions = language.file_extensions().to_owned();\\n    extensions.push(\\\"md\\\");\\n\\n    let (mut markdown, mut code) =\\n        Pipeline::from_loader(FileLoader::new(path).with_extensions(&extensions))\\n            .with_concurrency(50)\\n            .filter_cached(Redis::try_from_url(\\n                \\\"redis://localhost:6379\\\",\\n                \\\"swiftide-tutorial\\\",\\n            )?)\\n            .split_by(|node| {\\n                // Any errors at this point we just pass to 'markdown'\\n                let Ok(node) = node else { return true };\\n\\n                // On true we go 'markdown', on false we go 'code'.\\n                node.path.extension().map_or(true, |ext| ext == \\\"md\\\")\\n            });\"","\"use swiftide::{\\n    indexing::{\\n        loaders::FileLoader,\\n        transformers::{ChunkCode, ChunkMarkdown, Embed, MetadataQACode, MetadataQAText},\\n    },\\n    query::{\\n        self,\\n        answers::Simple,\\n        evaluators,\\n        query_transformers::{self, GenerateSubquestions},\\n        states, Query,\\n    },\\n};\\n\\nuse std::{path::PathBuf, str::FromStr};\\n\\nuse anyhow::{Context as _, Result};\\nuse clap::Parser;\\nuse indoc::formatdoc;\\nuse qdrant_client::qdrant::SearchPointsBuilder;\\nuse swiftide::{\\n    indexing::Pipeline,\\n    integrations::{openai::OpenAI, qdrant::Qdrant, redis::Redis, treesitter::SupportedLanguages},\\n};\\n\\n#[derive(Parser, Debug)]\\n#[command(version, about, long_about = None)]\\nstruct Args {\\n    #[arg(short, long)]\\n    language: String,\\n\\n    #[arg(short, long, default_value = \\\"./\\\")]\\n    path: PathBuf,\\n\\n    queries: Vec<String>,\\n}\\n\\n#[tokio::main]\"","\"\\nasync fn main() -> Result<()> {\\n    tracing_subscriber::fmt::init();\\n\\n    let args = Args::parse();\\n\\n    let openai = OpenAI::builder()\\n        .default_embed_model(\\\"text-embedding-3-small\\\")\\n        .default_prompt_model(\\\"gpt-3.5-turbo\\\")\\n        .build()?;\\n\\n    let qdrant = Qdrant::builder()\\n        .vector_size(1536)\\n        .collection_name(\\\"swiftide-ragas\\\")\\n        .build()?;\\n\\n    index_all(&args.language, &args.path, &openai, &qdrant).await?;\\n\\n    let openai = OpenAI::builder()\\n        .default_embed_model(\\\"text-embedding-3-small\\\")\\n        .default_prompt_model(\\\"gpt-4o\\\")\\n        .build()?;\\n\\n    let response = query(&openai, &qdrant, &args.queries).await?;\\n    println!(\\n        \\\"{}\\\",\\n        response\\n            .into_iter()\\n            .map(|q| q.answer().to_string())\\n            .collect::<Vec<_>>()\\n            .join(\\\"\\\\n\\\")\\n    );\\n\\n    Ok(())\\n}\\n\\nasync fn index_all(language: &str, path: &PathBuf, openai: &OpenAI, qdrant: &Qdrant) -> Result<()> {\\n    tracing::info!(path=?path, language, \\\"Indexing code\\\");\"","\"\\n\\n    code = code\\n        // Uses tree-sitter to extract best effort blocks of code. We still keep the minimum\\n        // fairly high and double the chunk size\\n        .then_chunk(ChunkCode::try_for_language_and_chunk_size(\\n            language,\\n            50..1024,\\n        )?)\\n        .then(MetadataQACode::new(openai.clone()));\\n\\n    markdown = markdown\\n        .then_chunk(ChunkMarkdown::from_chunk_range(50..1024))\\n        // Generate questions and answers and them to the metadata of the node\\n        .then(MetadataQAText::new(openai.clone()));\\n\\n    code.merge(markdown)\\n        .then_in_batch(50, Embed::new(openai.clone()))\\n        .then_store_with(qdrant.clone())\\n        .run()\\n        .await\\n}\"","\"\\n\\nasync fn query(\\n    openai: &OpenAI,\\n    qdrant: &Qdrant,\\n    questions: &[String],\\n) -> Result<Vec<Query<states::Answered>>> {\\n    let ragas = evaluators::ragas::Ragas::from_prepared_questions(questions);\\n\\n    let pipeline = query::Pipeline::default()\\n        .evaluate_with(ragas.clone())\\n        .then_transform_query(GenerateSubquestions::from_client(openai.clone()))\\n        .then_transform_query(query_transformers::Embed::from_client(openai.clone()))\\n        .then_retrieve(qdrant.clone())\\n        .then_answer(Simple::from_client(openai.clone()));\\n\\n    let answers = pipeline.query_all(ragas.questions().await).await;\\n\\n    let json = ragas.to_json().await;\\n    std::fs::write(\\\"ragas.json\\\", json).context(\\\"Failed to write ragas.json\\\")?;\\n\\n    answers\\n}\"","\"\\n\\nasync fn query(\\n    openai: &OpenAI,\\n    qdrant: &Qdrant,\\n    questions: &[String],\\n) -> Result<Vec<Query<states::Answered>>> {\\n    let ragas = evaluators::ragas::Ragas::from_prepared_questions(questions);\\n\\n    let pipeline = query::Pipeline::default()\\n        .evaluate_with(ragas.clone())\\n        .then_transform_query(GenerateSubquestions::from_client(openai.clone()))\\n        .then_transform_query(query_transformers::Embed::from_client(openai.clone()))\\n        .then_retrieve(qdrant.clone())\\n        .then_answer(Simple::from_client(openai.clone()));\\n\\n    let answers = pipeline.query_all(ragas.questions().await).await;\\n\\n    println!(\\\"{}\\\", ragas.to_json().await);\\n\\n    answers\\n}\""],"ground_truth":"","question":"How is swiftide used?"}]